//#1
import org.apache.spark.sql.SparkSession
//#2
import org.apache.log4j._
Logger.getLogger("org").setLevel(Level.ERROR)
//#3
val spark = SparkSession.builder().getOrCreate()
//#4
import org.apache.spark.ml.clustering.KMeans
//#5
val data  = spark.read.option("header","true").option("inferSchema", "true").format("csv").load("Wholesale customers data.csv")

data.printSchema

data.head(1)

//#6
val colnames = data.columns
val firstrow = data.head(1)(0)
println("\n")
println("Example data row")
for(ind <- Range(1, colnames.length)){
  println(colnames(ind))
  println(firstrow(ind))
  println("\n")
}


val Entretenimiento = (data.select(data("feacture_data").as("label"), $"Fresh", $"Milk",
  $"Grocery", $"Frozen", $"Detergents_paper"))

//#7
import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, VectorIndexer, OneHotEncoder}
import org.apache.spark.ml.linalg.Vectors

//#8
val assembler = new VectorAssembler().setInputCols(Array("Channel", "Region", "Delicassen"))
//#9
val output = assembler.transform(data).select($"label", $"features")

output.show()

//#10
val kmeans = new KMeans().setK(3).setSeed(1L)
val model = kmeans.fit(data)

//#11
val WSSE = model.computeCost(data)
println(s"Within set sum of Squared Errors = $WSSE")

//#12

println("Cluster Centers: ")
model.clusterCenters.foreach(println)
